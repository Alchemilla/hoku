\section{Benchmark Data Generation}
To analyze a set of algorithms, we need images that we know \textbf{everything} about: which stars in the image are which, the field of view of the camera, where exactly in the sky this image refers to, what kind of error exists in this image, etc... As with every experiment, we want to hold as much as we can constant while varying what we need. \smallskip

Unfortunately, obtaining actual images with error we can control is incredibly difficult. If we are trying to capture an image without error, we have to control the lens distortion and cross check our catalog with the image manually to make sure that this image is accurate. Couple this with the fact that we are stuck to a fixed field of view and image size, this significantly restricts our testing. My solution is to remove reliance on the camera itself to capture images, and generate images ourselves. By doing so, we can produce an image that accurately represents a portion of the sky (as far as the implementations are concerned) and control which error we introduce ourselves. \smallskip

\subsection{Yale Bright Star Catalog (BSC5)}
First things first, we require a data set to generate our images from. I will be using the Yale Bright Star set, an astronomical catalog of recorded stars and their position relative to the Earth. The BSC (Bright Star Catalog) contains roughly 5000 stars visible to the naked eye from Earth. There exists other catalog which provide a significantly larger data set, but many of these entries require special instruments to detect and aren't very practical to keep. ~5000 is a \textit{generous} lower bound on what stars we will likely encounter through actual implementation. \smallskip

Entries are stored in terms of right ascension (hours, minutes, and seconds) and declination (degrees, arc-minutes, arc-seconds). For the purpose of this project, each star was projected onto a sphere with $r=1$. The Cartesian coordinates for all entries were then stored in a database, along with the Harvard revised number for identification. This projection allows us to skip a mapping step which may contribute to positional error of the stars themselves, a variable we want to control ourselves. The problem of mapping also stems back to the camera hardware itself, which is not the focus here. \smallskip

\subsection{Input Data Generation}
Each algorithm will be given a list of three-dimensional points representing stars in the image, as well as the field-of-view. The field-of-view is the only hardware constraint we are concerned with. For the star identification methods we are analyzing, $f$ is used as a constraint to eliminate results. Below are some defining characteristics of every star $s$ in our image $S$.

\begin{itemize}
\item There exists no star whose angle of separation $\theta > fov$. 
\item The angle of separation to the image center $\theta_{s-f} <= \frac{fov}{2}$. $f$ is not given.
\item A rotation $q$ is applied, which is used for all stars in $S$. $q$ is not given.
\item The Harvard revised number (identification number) is $label = 0$.
\item The origin is $(0, 0, 0)$.
\item The vector length is $r=1$.
\end{itemize}

The pseudocode \texttt{GenerateCleanInput} generates 

The pseudocode \texttt{GenerateCleanInputData()} generates X-Y coordinate values for stars in the FOV $f$ with frame size $q$ at an equatorial point $P = (\alpha, \delta)$ tilted $r$ degrees. The catalog $C$ holds the $\alpha$ and $\delta$ values for each star.

\begin{lstlisting}[breaklines=true, showspaces=false]
GenerateCleanInputData(C, f, q, r, P):

	S = blank image
	
	S.center maps directly to P
 
	a = stars in C within the bounds of f
	                    
	a' = stars in a converted to image coordinates
	
	points in a' rotated by r degrees -> S
	
	return S
\end{lstlisting}

\subsection{Error Models}
With the input data generated, we can now simulate environmental error by altering the image. 
